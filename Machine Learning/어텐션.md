# 어텐션

---

# 어텐션(Attention)이란?

자연어 처리의 기존 모델 seq2seq에서는 인코더가 입력 시퀀스를 context vector로 압축하고, 디코더는 이 벡터를 통해서 출력 시퀀스를 만들어낸다.
하지만, 이렇게 고정된 사이즈의 vector로 압축해 사용하는 모델은 문제점이 존재한다.

고정된 사이즈의 vector에 모든 정보를 저장하기 때문에 정보 손실이 일어난다.
RNN의 고질적인 문제인 기울기 소실 문제가 존재한다.
이는 자연어 처리에서 입력 시퀀스가 길어질수록 성능이 떨어지는 현상으로 나타난다. 이를 해결하기 위해 제안된 기법이 **어텐션(Attention)** 기법이다.

어텐션의 기본 아이디어는 다음 단어를 예측하는 매 시점마다 입력 시퀀스를 다시 참고하는 것이다.
이 때, 동일한 비중으로 참고하는 것이 아니라 관련이 있는 단어에 치중해서 본다.
참고할 단어간 연관성 가중치를 **Attention Weight**라고 한다.

### 어텐션의 핵심 개념 3개

- Query(Q): 알고 싶은 정보, 질문 -> “지금 내가 어떤 정보를 찾고 있는가?”를 표현한 벡터.
- Key(K): 문장 속 단어의 특징, 단서
- Value(V): 각 단어가 자체가 가지고 있는 의미

흐름은 아래와 같다.

1. Q와 K를 비교해서 어떤 단어가 질문과 관련이 깊은지 점수를 매긴다.
   - 가중치 벡터 내적을 통해 구하고 업데이트 한다.
   - 큰 차원에서 극단적인 값이 나오는 문제를 해결하기위해 $\sqrt{차원 수}$로 나누어 준다.
2. 이 점수를 Softmax 함수로 졍규화 해서 가중치로 만든다.
   - 영향을 주면 안되는 특정 위치의 token은 가중치를 음의 무한대로 설정하여 마스킹을 진행한다.
3. 각 단어 자체 의미인 V에 가중치를 곱해서 관련 정보를 모은다.
   - 기존 단어 의미에 문맥 정보가 포함되어 변화가 생긴다.

### 수학 중심의 연산 흐름

1. 유사도 계산
   Query와 Key 벡터 간의 내적을 통해 원시 점수를 얻는다.

   - $scores=QK^⊤$

2. 스케일링
   점곱값이 차원 수에 따라 커지는 걸 방지하기 위해 $\sqrt{d_k}$ 나눈다

   - $scores=\frac{QK^⊤}{\sqrt{d_k}}$

3. 정규화
   각 쿼리에 대해 키들 간의 상대 확률 분포를 얻는다.

   - $scores=softmax(\frac{QK^⊤}{\sqrt{d_k}})$

4. 가중합
   가중치 행렬로 Value를 가중합해서 최종 출력을 얻는다.

   - $scores=softmax(\frac{QK^⊤}{\sqrt{d_k}})V$

### 효과

- RNN과 달리 긴 문장에서 중요 단어를 잘 파악할 수 있다.
- 병렬 처리가 가능해서 속도가 빨라, Transformer의 핵심 기술로 활용되어 언어 모델 학습에 효과적이다.
- 번역, 문장 요약, 음성 인식, 추천 시스템 등 다양한 분야에 활용이 가능하다.

---

## 추가 질문
